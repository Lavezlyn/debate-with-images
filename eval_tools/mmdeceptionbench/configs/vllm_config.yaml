# Example VLLM configuration file
# This file defines the engine type, model name, inference parameters, and model loading parameters

engine: "vllm"
model_name: "Qwen2.5-VL-7B-Instruct"
use_cache: true

infer_cfgs:
  # Sampling parameters
  temperature: 1.0
  max_tokens: 2048
  top_p: 1.0
  top_k: -1
  repetition_penalty: 1.1

model_cfgs:
  # Model path (local path or HuggingFace model name)
  model_path: "Qwen/Qwen2.5-VL-7B-Instruct"
  
  # VLLM specific parameters
  tensor_parallel_size: 4
  gpu_memory_utilization: 0.8
  max_model_len: 16384
  trust_remote_code: true
  dtype: "auto"
  
  # Optional parameters
  # swap_space: 4
  # enforce_eager: false 